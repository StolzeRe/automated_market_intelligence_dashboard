{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import everything we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import psycopg2 # Needed to get database errors when uploading dataframe\n",
    "import sql\n",
    "import sqlalchemy\n",
    "import warnings # hides warning messages\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary for competitors & locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = { \n",
    "\n",
    "'Air France' : ['Mexico', 'Netherlands', 'China', 'Vietnam', 'Thailand', 'Indonesia', \n",
    "                'Malaysia', 'Morocco', 'Tunisia', \n",
    "                'France'\n",
    "                ],\n",
    "\n",
    "'Delta TechOps' : ['United States', 'Canada', 'Mexico', 'Argentina', 'Bolivia', 'Brazil', 'Chile', \n",
    "                    'Colombia', 'Ecuador', 'Guyana', 'Paraguay', \n",
    "                    'Peru', 'South Georgia', \n",
    "                    'Suriname', 'Uruguay', 'Venezuela'],\n",
    "                   \n",
    "'MTU Maintenance' : ['China', 'Serbia', 'Poland', 'Canada', 'United States', 'Malaysia', 'Germany'],\n",
    "\n",
    "'Lufthansa' : ['Germany', 'Hungary', 'Bulgaria'],\n",
    "\n",
    "'Safran' : ['Belgium', 'France', 'Dubai', 'Mexico', 'United Kingdom', 'Germany']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looped request from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functions_for_jobs import clean_location \n",
    "from functions_for_jobs import create_city_state_country \n",
    "from functions_for_jobs import assign_world_regions\n",
    "from functions_for_jobs import create_id\n",
    "from functions_for_jobs import translate_job_titles\n",
    "from functions_for_jobs import clean_job_titles\n",
    "from functions_for_jobs import clean_text\n",
    "from functions_for_jobs import translate_job_description\n",
    "from functions_for_jobs import delete_columns\n",
    "\n",
    "\n",
    "\n",
    "url = \"https://linkedin-jobs-search.p.rapidapi.com/\"\n",
    "headers = {\n",
    "            \"content-type\": \"application/json\",\n",
    "            \"X-RapidAPI-Host\": \"linkedin-jobs-search.p.rapidapi.com\",\n",
    "            \"X-RapidAPI-Key\": \"key\"\n",
    "            }\n",
    "i = 1\n",
    "for competitor in search_term:  \n",
    "\n",
    "    for search_country in search_term[competitor]:\n",
    "        \n",
    "        while i < 15:\n",
    "### Getting Data via API    \n",
    "            payload = {\n",
    "                        \"search_terms\": f'{competitor}',\n",
    "                        \"location\": f'{search_country}', \t\t\t\t\t\t\t\t\t        \n",
    "                        \"page\": f'{i}',     \t\t\t\t\t\t\t\t\t\t        \n",
    "                        \"fetch_full_text\": \"yes\"\n",
    "                        }\n",
    "           \n",
    "            response = requests.request(\"POST\", url, json=payload, headers=headers)\n",
    "            jobs_list = json.loads(response.content)\n",
    "            json.dumps(jobs_list, indent=3)\n",
    "            job = pd.json_normalize(jobs_list, sep=\"_\")\n",
    "            print(job)\n",
    "            print(competitor)\n",
    "            print(search_country)\n",
    "            time.sleep(15)   \n",
    "            print('slept well and ready to go on')                                                                      # just 5 requests per minute are allowed\n",
    "        \n",
    "\n",
    "            if len(job) > 1:         \n",
    "### Cleaning\n",
    "                \n",
    "                jobs = job[['company_name', 'job_title','job_location','posted_date','full_text']] # create DataFrame with necessary columns\n",
    "                jobs['search_country'] = search_country              \n",
    "                jobs = clean_location(jobs)\n",
    "                jobs = create_city_state_country(jobs)\n",
    "                jobs = assign_world_regions(jobs)\n",
    "                jobs = create_id(jobs)\n",
    "                jobs = translate_job_titles(jobs)\n",
    "                jobs = clean_job_titles(jobs)\n",
    "                jobs = clean_text(jobs)\n",
    "                jobs['posted_date'] = pd.to_datetime(jobs['posted_date'], format=\"%Y-%m-%d\")\n",
    "                jobs = translate_job_description(jobs)\n",
    "                jobs = delete_columns(jobs)\n",
    "                \n",
    "                \n",
    "                \n",
    "### Upload\n",
    "                schema = os.getenv('schema')\n",
    "                from sql import engine\n",
    "                for row in range(len(jobs)):\n",
    "                    try:\n",
    "                        jobs.iloc[row:row+1, :].to_sql(name='jobs',                     # Name of DF table\n",
    "                                                        con=engine,                     # Engine or connection\n",
    "                                                        if_exists='append',             # Just add new values in existing table\n",
    "                                                        schema=schema,                  # Use schema that was defined earlier in .sql\n",
    "                                                        index=False,                    # Write DataFrame index as a column\n",
    "                                                        chunksize=1,                    # Specify the number of rows in each batch to be written at a time\n",
    "                                                        method='multi')                 # Pass multiple values in a single INSERT clause\n",
    "                        print(f\"row {row} of jobs was inserted successfully.\")\n",
    "                    except:\n",
    "                        print(f\"row {row} of jobs was ignored.\")\n",
    "                        continue\n",
    "                i += 1 \n",
    "                              \n",
    "            else:\n",
    "                i=1\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Request from API\n",
    "\n",
    "search_term, location and page have to be assigned manually"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43c12cbf984401a473fdf5ad54be096b36364cb85034417d000695974821cc3f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('sql-practice')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
